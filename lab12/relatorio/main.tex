\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{color}
\usepackage{float}
\usepackage{multirow}
\usepackage{hyperref}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
\lstset{language=Python}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Relatório do Laboratório 12: \\ Aprendizado por Reforço Livre de Modelo\\
}

\author{\IEEEauthorblockN{Isabelle Ferreira de Oliveira}
\IEEEauthorblockA{\textit{CT-213 - Engenharia da Computação 2020} \\
\textit{Instituto Tecnológico de Aeronáutica (ITA)}\\
São José dos Campos, Brasil \\
isabelle.ferreira3000@gmail.com}
}

\maketitle

\begin{abstract}
Esse relatório documenta a implementação de algoritmos de Aprendizado por Reforço (RL) Livre de Modelo, a saber Sarsa e Q-Learning, e utilizá-los para resolver o problema do robô seguidor de linha.
\end{abstract}

\begin{IEEEkeywords}
Aprendizado por reforço, Sarsa, Q-Learning
\end{IEEEkeywords}

\section{Implementação}

\subsection{Implementação dos algoritmos de RL}

\subsubsection{Função epsilon\underline{\space}greedy\underline{\space}action}

\subsubsection{Função greedy\underline{\space}action}

\subsubsection{Função get\underline{\space}greedy\underline{\space}action para algoritmo Sarsa}

\subsubsection{Função learn para algoritmo Sarsa}

\subsubsection{Função get\underline{\space}greedy\underline{\space}action para algoritmo Q-Learning}

\subsubsection{Função learn para algoritmo Q-learning}

Essa parte do laboratório se tratava da implementação da função \textit{policy\underline{\space}evaluation()}, presente no arquivo \textit{dynamic\underline{\space}programming.py}, fornecido pelo código base do professor. 

De maneira simples, essa função consiste em codificar a equação: $v_{k+1}\left ( s \right ) = \sum_{a\in A} \pi \left ( a|s \right ) r \left ( s, a \right ) + \gamma \sum_{a\in A} \sum_{s'\in S} \pi \left ( a|s \right ) p \left ( s' | s, a \right ) v_k \left ( s' \right )$, apresentada de forma bem semelhante no roteiro do laboratório \cite{roteiro}.

Para implementá-la, os estados $s$ se tornaram tuplas $(i, j)$, que foram iteradas por todo o grid world. Foram feitos loops também para iterar pelas ações $a$ e pelos possíveis próximos estados $s'$, e os resultados da equação acima foram somados ao valor associado ao estado $s$ em que se estava. 

\begin{enumerate}
\item $\pi \left ( a|s \right )$ era encontrado em \textit{policy};

\item $r \left ( s, a \right )$ era encontrado em \textit{grid\underline{\space}world.reward()};

\item $p \left ( s' | s, a \right )$ era encontrado em \textit{grid\underline{\space}world.transition\underline{\space}probability()};

\item $v_k \left ( s' \right )$ era a \textit{policy} para um próximo estado $s'$, encontrado iterando-se sobre \textit{grid\underline{\space}world.get\underline{\space}valid\underline{\space}sucessors()}.
\end{enumerate}

Vale ressaltar que, após um número definido previamente de iterações, ou após a convergência dos valores de $v_{k+1}(s)$, o loop era interrompido.

\subsection{Aprendizado da política do robô seguidor de linha}
Já essa parte tratava-se da implementação da função \textit{value\underline{\space}iteration()}, também presente no arquivo \textit{dynamic\underline{\space}programming.py}, fornecido pelo código base do professor.

Análogo a função anterior, essa função consiste na codificação da equação: $v_{k+1}\left ( s \right ) = max_{a \in A} \left (  r \left ( s, a \right ) + \gamma \sum_{s'\in S} p \left ( s' | s, a \right ) v_k \left ( s' \right ) \right )$, também presente no roteiro do laboratório \cite{roteiro}.

A implementação também se tornou bastante semelhante à função de Avaliação de Política, com os estados $s$ sendo tuplas $(i, j)$, que foram iteradas por todo o grid world, e loops para iterar pelas ações $a$ e pelos possíveis próximos estados $s'$, dessa vez buscando os valores máximos dos resultados da equação, para serem considerados como valor associado ao estado $s$ em que se estava nessa situação. 

\begin{enumerate}
\item $r \left ( s, a \right )$ estava em \textit{grid\underline{\space}world.reward()};

\item $p \left ( s' | s, a \right )$ estava em \textit{grid\underline{\space}world.transition\underline{\space}probability()};

\item $v_k \left ( s' \right )$ era a \textit{policy} para um próximo estado $s'$, encontrado iterando-se sobre \textit{grid\underline{\space}world.get\underline{\space}valid\underline{\space}sucessors()}.
\end{enumerate}

Vale ressaltar novamente que, após um número definido previamente de iterações, ou após a convergência dos valores de $v_{k+1}(s)$, o loop também era interrompido.

\section{Resultados e Conclusões}

\subsection{Primeiro Grid World}
Foram gerados os resultados para os parâmetros de Grid World abaixo:

\begin{enumerate}
\item CORRECT\underline{\space}ACTION\underline{\space}PROB = 1.0

\item GAMMA = 1.0
\end{enumerate}

Primeiro comparando-se os resultados apresentados nas Figuras \ref{grid1/value_iteration} e \ref{grid1/policy_iteration}, é possível notar que eles são idênticos, o que é esperado, uma vez que ambas as técnicas levam a convergência dos valores corretos de \textit{policy} e \textit{value}.

Sobre a Figura \ref{grid1/policy_evaluation}, a tendência observada é o \textit{value} calculado ser maior em módulo para estados mais distantes do estado objetivo.

Nos três resultados é possível notar o \textit{value} 0.0 para o estado objetivo, o que também condiz com o esperado. 

\begin{figure}[htbp]
\centering
\centerline{\includegraphics[scale=0.5]{imagens/grid1/policy_evaluation.png}}
\caption{Resultado observado para o teste da \textit{policy\underline{\space}evaluation()}, para a primeira opção de Grid World.}.
\label{grid1/policy_evaluation}
\end{figure}

\begin{figure}[htbp]
\centering
\centerline{\includegraphics[scale=0.5]{imagens/grid1/value_iteration.png}}
\caption{Resultado observado para o teste da \textit{value\underline{\space}iteration()}, para a primeira opção de Grid World.}.
\label{grid1/value_iteration}
\end{figure} 

\begin{figure}[htbp]
\centering
\centerline{\includegraphics[scale=0.5]{imagens/grid1/policy_iteration.png}}
\caption{Resultado observado para o teste da \textit{policy\underline{\space}iteration()}, para a primeira opção de Grid World.}.
\label{grid1/policy_iteration}
\end{figure}

\subsection{Segundo Grid World}
Foram gerados os resultados para os parâmetros de Grid World abaixo:

\begin{enumerate}
\item CORRECT\underline{\space}ACTION\underline{\space}PROB = 0.8

\item GAMMA = 0.98
\end{enumerate}

Primeiro comparando-se os resultados apresentados nas Figuras \ref{grid2/value_iteration} e \ref{grid2/policy_iteration}, também é possível notar que eles são idênticos, o que é novamente esperado, uma vez que ambas as técnicas levam a convergência dos valores corretos de \textit{policy} e \textit{value}.

Sobre a Figura \ref{grid2/policy_evaluation}, a mesma tendência que no primeiro Grid World é observada, ou seja, o \textit{value} calculado é maior em módulo para estados mais distantes do estado objetivo.

Nos três resultados também é possível notar o \textit{value} 0.0 para o estado objetivo, o que também condiz com o esperado. 

\begin{figure}[htbp]
\centering
\centerline{\includegraphics[scale=0.5]{imagens/grid2/policy_evaluation.png}}
\caption{Resultado observado para o teste da \textit{policy\underline{\space}evaluation()}, para a segunda opção de Grid World.}.
\label{grid2/policy_evaluation}
\end{figure}

\begin{figure}[htbp]
\centering
\centerline{\includegraphics[scale=0.5]{imagens/grid2/value_iteration.png}}
\caption{Resultado observado para o teste da \textit{value\underline{\space}iteration()}, para a segunda opção de Grid World.}.
\label{grid2/value_iteration}
\end{figure}

\begin{figure}[htbp]
\centering
\centerline{\includegraphics[scale=0.5]{imagens/grid2/policy_iteration.png}}
\caption{Resultado observado para o teste da \textit{policy\underline{\space}iteration()}, para a segunda opção de Grid World.}
\label{grid2/policy_iteration}
\end{figure}

Por fim, comparando-se as duas situações de Grid World, é possível notar que, com a adição do desconto \textit{GAMMA}, e agora com a probabilidade de o agente executar uma ação diferente da escolhida para cada estado, tem-se que os \textit{value} referentes a cada estado são maiores em módulo do que os calculados na primeira situação.

Isso se justifica e condiz com o esperado, uma vez que não se sabendo deterministicamente a ação tomada em cada estado, a função valor entende esse estado como "pior" quando comparado a situação na qual \textit{CORRECT\underline{\space}ACTION\underline{\space}PROB} = 1. Além disso, o fator \textit{GAMMA} adiciona mais imediatismo à recompensa das ações do agente, o que também diminui a medida de quão "bom" é determinado estado em comparação a situação no qual todas as recompensas até o objetivo são igualmente contabilizadas.

\begin{thebibliography}{00}
\bibitem{roteiro} M. Maximo, ``Roteiro: Laboratório 11 - Programação Dinâmica''. Instituto Tecnológico de Aeronáutica, Departamento de Computação. CT-213, 2019.
\end{thebibliography}

\end{document}
